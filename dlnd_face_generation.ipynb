{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Geração de Rosto\n",
    "Neste projeto, você usará redes adversárias geradoras para gerar novas imagens de faces.\n",
    "### Obtenha os dados\n",
    "Você estará usando dois conjuntos de dados neste projeto:\n",
    "- MNIST\n",
    "- CelebA\n",
    "\n",
    "Como o conjunto de dados celebA é complexo e você está fazendo GANs em um projeto pela primeira vez, queremos que você teste sua rede neural no MNIST antes da CelebA. Executar as GANs no MNIST permitirá que você veja o quão bem seu modelo treina mais cedo.\n",
    "\n",
    "Se você estiver usando o [FloydHub] (https://www.floydhub.com/), defina `data_dir` como\" / input \"e use o [FloydHub data ID] (http://docs.floydhub.com/home / using_datasets /) \"R5KrjnANiKVhLWAkpXhNBe\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found mnist Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading celeba:   2%|▉                                                     | 25.7M/1.44G [16:15<14:57:30, 26.3kB/s]\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "[WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4a982fdffca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mnist'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'celeba'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\dudu_\\Downloads\\Documentos\\Doucuments\\Udacity\\Projeto 5 - face-generation\\helper.py\u001b[0m in \u001b[0;36mdownload_extract\u001b[0;34m(database_name, data_path)\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 pbar.hook)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mhash_code\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dudu_\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dudu_\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dudu_\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dudu_\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dudu_\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m    927\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                   self.__class__)\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dudu_\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dudu_\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \"\"\"\n\u001b[1;32m    574\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto"
     ]
    }
   ],
   "source": [
    "data_dir = './data'\n",
    "\n",
    "import helper\n",
    "\n",
    "helper.download_extract('mnist', data_dir)\n",
    "helper.download_extract('celeba', data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore os dados\n",
    "### MNIST\n",
    "Como você sabe, o conjunto de dados [MNIST] (http://yann.lecun.com/exdb/mnist/) contém imagens de dígitos manuscritos. Você pode ver o primeiro número de exemplos mudando `show_n_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "show_n_images = 25\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from matplotlib import pyplot\n",
    "\n",
    "mnist_images = helper.get_batch(glob(os.path.join(data_dir, 'mnist/*.jpg'))[:show_n_images], 28, 28, 'L')\n",
    "pyplot.imshow(helper.images_square_grid(mnist_images, 'L'), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### CelebA\n",
    "O conjunto de dados [CelebA Attributes Dataset (CelebA)] (http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) contém mais de 200.000 imagens de celebridades com anotações. Como você vai gerar faces, não precisará das anotações. Você pode ver o primeiro número de exemplos mudando `show_n_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "show_n_images = 25\n",
    "\n",
    "mnist_images = helper.get_batch(glob(os.path.join(data_dir, 'img_align_celeba/*.jpg'))[:show_n_images], 28, 28, 'RGB')\n",
    "pyplot.imshow(helper.images_square_grid(mnist_images, 'RGB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pré-processar os dados\n",
    "Como o foco principal do projeto é construir as GANs, nós pré-processaremos os dados para você. Os valores do conjunto de dados MNIST e CelebA estarão no intervalo de -0,5 a 0,5 de 28x28 imagens dimensionais. As imagens da CelebA serão cortadas para remover partes da imagem que não incluem um rosto e redimensionadas para 28x28.\n",
    "\n",
    "As imagens MNIST são imagens em preto e branco com um único [canal de cor] (https://en.wikipedia.org/wiki/Channel_ (digital_image% 29), enquanto as imagens da CelebA têm [3 canais de cores (canal de cores RGB)] ( https://en.wikipedia.org/wiki/Channel_(digital_image%29#RGB_Images).\n",
    "## Construa a Rede Neural\n",
    "Você construirá os componentes necessários para construir uma GAN implementando as seguintes funções abaixo:\n",
    "- `model_inputs`\n",
    "- `discriminador`\n",
    "- `gerador`\n",
    "- `model_loss`\n",
    "- `model_opt`\n",
    "- `train`\n",
    "\n",
    "### Verifique a versão do TensorFlow e acesse o GPU\n",
    "Isso irá verificar se você tem a versão correta do TensorFlow e acesso a uma GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Verificar versão do Tensorflow\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Checar GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('GPU Não encontrada. Por favor utilize uma GPU para treinar sua rede neural.')\n",
    "else:\n",
    "    print('GPU: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Entrada\n",
    "Implemente a função `model_inputs` para criar espaços reservados para TF para a rede neural. Deve criar os seguintes marcadores de posição:\n",
    "- Espaçador de imagens de entrada real com classificação 4 usando `image_width`,` image_height` e `image_channels`.\n",
    "- Espaço reservado de entrada Z com classificação 2 usando `z_dim`.\n",
    "- Espaço reservado para a taxa de aprendizagem com classificação 0.\n",
    "\n",
    "Retorna os marcadores de posição a seguir da tupla (tensor de imagens de entrada reais, tensor de dados z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import problem_unittests as tests\n",
    "\n",
    "def model_inputs(image_width, image_height, image_channels, z_dim):\n",
    "    input_feat = tf.placeholder(dtype=tf.float32, shape=(None, image_width, image_height, image_channels),\n",
    "                              name='r_input')\n",
    "    input_dim = tf.placeholder(dtype=tf.float32,shape=(None, z_dim), name='z_input')\n",
    "    learning_rate = tf.placeholder(dtype=tf.float32, name='lr')\n",
    "\n",
    "    return input_feat, input_dim, learning_rate\n",
    "\n",
    "tests.test_model_inputs(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Discriminador\n",
    "Implemente `discriminator` para criar uma rede neural discriminadora que discrimine` images`. Esta função deve ser capaz de reutilizar as variáveis ​​na rede neural. Use [`tf.variable_scope`] (https://www.tensorflow.org/api_docs/python/tf/variable_scope) com um nome de escopo de\" discriminador \"para permitir que as variáveis ​​sejam reutilizadas. A função deve retornar uma tupla de (tensor output do discriminador, tensor logits do discriminador)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def discriminator(images, reuse=False):\n",
    "    \n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        hl1 = tf.layers.conv2d(images, 64, 5, strides=2, padding='same')\n",
    "        hl1 = tf.maximum(alpha * hl1, hl1)\n",
    "        \n",
    "        hl2 = tf.layers.conv2d(hl1, 128, 5, strides=2, padding='same')\n",
    "        hl2 = tf.layers.batch_normalization(hl2, training=True)\n",
    "        hl2 = tf.maximum(alpha * hl2, hl2)\n",
    "        \n",
    "        hl3 = tf.layers.conv2d(hl2, 256, 5, strides=2, padding='same')\n",
    "        hl3 = tf.layers.batch_normalization(hl3, training=True)\n",
    "        hl3 = tf.maximum(alpha * hl3, hl3)\n",
    "\n",
    "        inp_f = tf.reshape(h3, (-1, 4*4*256))\n",
    "        dl = tf.layers.dropout(inputs=inp_f, rate=dropout)\n",
    "        logits = tf.layers.dense(dl, 1)\n",
    "        output = tf.sigmoid(logits)\n",
    "        \n",
    "    return output, logits\n",
    "\n",
    "tests.test_discriminator(discriminator, tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gerador\n",
    "Implemente `generator` para gerar uma imagem usando` z`. Esta função deve ser capaz de reutilizar as variáveis ​​na rede neural. Use [`tf.variable_scope`] (https://www.tensorflow.org/api_docs/python/tf/variable_scope) com um nome de escopo de\" generator \"para permitir que as variáveis ​​sejam reutilizadas. A função deve retornar as imagens 28 x 28 x `out_channel_dim` geradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generator(z, out_channel_dim, is_train=True):\n",
    "    with tf.variable_scope('generator', reuse= not is_train):\n",
    "        hl1 = tf.layers.dense(z, 3*3*512) # Dimensões\n",
    "        hl1 = tf.reshape(h1, (-1, 3, 3, 512)) # Novo shape\n",
    "        hl1 = tf.layers.batch_normalization(hl1, training=is_train)\n",
    "        hl1 = tf.maximum(alpha * hl1, hl1)\n",
    "        \n",
    "        hl2 = tf.layers.conv2d_transpose(hl1, 256, kernel_size=4, strides=2, padding='same')\n",
    "        hl2 = tf.layers.batch_normalization(hl2, training=is_train)\n",
    "        hl2 = tf.maximum(alpha * hl2, hl2)\n",
    "        \n",
    "        hl3 = tf.layers.conv2d_transpose(hl2, 128, kernel_size=4, strides=2, padding='valid')\n",
    "        hl3 = tf.layers.batch_normalization(hl3, training=is_train)\n",
    "        hl3 = tf.maximum(alpha * hl3, hl3)\n",
    "        \n",
    "        logits = tf.layers.conv2d_transpose(hl3, out_channel_dim, kernel_size=5,strides=2, padding='same')\n",
    "        output = tf.tanh(logits)\n",
    "        \n",
    "        return output\n",
    "\n",
    "tests.test_generator(generator, tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Perda\n",
    "Implemente `model_loss` para construir as GANs para treinamento e calcular a perda. A função deve retornar uma tupla de (perda do discriminador, perda do gerador). Use as seguintes funções que você implementou:\n",
    "- `discriminador (imagens, reutilização = Falso)`\n",
    "- `gerador (z, out_channel_dim, is_train = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, out_channel_dim):\n",
    "\n",
    "    generator = generator(input_z, out_channel_dim, is_train=True, alpha=alpha)\n",
    "    discriminator, r_logits = discriminator(input_real, reuse=False, alpha=alpha, dropout=dropout)\n",
    "    f_model, f_logits = discriminator(generator, reuse=True, alpha=alpha, dropout=dropout)\n",
    "\n",
    "    r_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_r, \n",
    "                                                labels=tf.ones_like(discriminator)*(1 - smooth)))\n",
    "    f_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits, labels=tf.zeros_like(f_model)))\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits, labels=tf.ones_like(f_model)))\n",
    "\n",
    "    d_loss = r_loss + f_loss\n",
    "\n",
    "    return d_loss, g_loss\n",
    "    return None, None\n",
    "\n",
    "tests.test_model_loss(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Otimização\n",
    "Implemente `model_opt` para criar as operações de otimização para as GANs. Use [`tf.trainable_variables`] (https://www.tensorflow.org/api_docs/python/tf/trainable_variables) para obter todas as variáveis ​​treináveis. Filtre as variáveis ​​com nomes que estão nos nomes do escopo do discriminador e do gerador. A função deve retornar uma tupla de (operação de treinamento do discriminador, operação de treinamento do gerador)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, learning_rate, beta1):\n",
    "    train_var = tf.trainable_variables()\n",
    "    discriminator_var = [var for var in train_var if var.name.startswith('discriminator')]\n",
    "    generator_var = [var for var in train_var if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        dis_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=discriminator_var)\n",
    "        gen_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=generator_var)\n",
    "\n",
    "    return d_train_opt, g_train_opt\n",
    "\n",
    "tests.test_model_opt(model_opt, tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Treinamento em Redes Neurais\n",
    "### Show Output\n",
    "Use esta função para mostrar a saída atual do gerador durante o treinamento. Isso ajudará você a determinar quão bem as GANs estão treinando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def show_generator_output(sess, n_images, input_z, out_channel_dim, image_mode):\n",
    "    cmap = None if image_mode == 'RGB' else 'gray'\n",
    "    z_dim = input_z.get_shape().as_list()[-1]\n",
    "    example_z = np.random.uniform(-1, 1, size=[n_images, z_dim])\n",
    "\n",
    "    samples = sess.run(\n",
    "        generator(input_z, out_channel_dim, False),\n",
    "        feed_dict={input_z: example_z})\n",
    "\n",
    "    images_grid = helper.images_square_grid(samples, image_mode)\n",
    "    pyplot.imshow(images_grid, cmap=cmap)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Treinar\n",
    "Implementar o `trem` para construir e treinar as GANs. Use as seguintes funções que você implementou:\n",
    "- `model_inputs (image_width, image_height, image_channels, z_dim)`\n",
    "- `model_loss (input_real, input_z, out_channel_dim)`\n",
    "- `model_opt (d_loss, g_loss, learning_rate, beta1)`\n",
    "\n",
    "Use o `show_generator_output` para mostrar a saída do` generator` enquanto você treina. Executar `show_generator_output` para cada lote aumentará drasticamente o tempo de treinamento e aumentará o tamanho do bloco de anotações. É recomendado imprimir a saída do `gerador` a cada 100 lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(epoch_count, batch_size, z_dim, learning_rate, beta1, get_batches, data_shape, data_image_mode):\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch_i in range(epoch_count):\n",
    "            for batch_images in get_batches(batch_size):\n",
    "                # TODO: Train Model\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### MNIST\n",
    "Teste sua arquitetura de GANs no MNIST. Depois de 2 épocas, as GANs devem poder gerar imagens que se pareçam com dígitos manuscritos. Certifique-se de que a perda do gerador seja menor que a perda do discriminador ou próxima de 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = None\n",
    "z_dim = None\n",
    "learning_rate = None\n",
    "beta1 = None\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "mnist_dataset = helper.Dataset('mnist', glob(os.path.join(data_dir, 'mnist/*.jpg')))\n",
    "with tf.Graph().as_default():\n",
    "    train(epochs, batch_size, z_dim, learning_rate, beta1, mnist_dataset.get_batches,\n",
    "          mnist_dataset.shape, mnist_dataset.image_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### CelebA\n",
    "Execute suas GANs na CelebA. Demora cerca de 20 minutos na GPU média para executar uma época. Você pode executar toda a época ou parar quando começar a gerar faces realistas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = None\n",
    "z_dim = None\n",
    "learning_rate = None\n",
    "beta1 = None\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "celeba_dataset = helper.Dataset('celeba', glob(os.path.join(data_dir, 'img_align_celeba/*.jpg')))\n",
    "with tf.Graph().as_default():\n",
    "    train(epochs, batch_size, z_dim, learning_rate, beta1, celeba_dataset.get_batches,\n",
    "          celeba_dataset.shape, celeba_dataset.image_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Enviando este projeto\n",
    "Ao enviar este projeto, certifique-se de executar todas as células antes de salvar o bloco de anotações. Salve o arquivo do notebook como \"dlnd_face_generation.ipynb\" e salve-o como um arquivo HTML em \"File\" -> \"Download as\". Inclua os arquivos \"helper.py\" e \"problem_unittests.py\" no seu envio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
